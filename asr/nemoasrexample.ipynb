{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bit374105b89564480f80bd453b07d02b3f",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # NVIDIA NeMo - Building Custom Speech Recognition Model\n",
    "\n",
    "1.[Introduction](Introduction)\n",
    "\n",
    "2.[Setup](Setup)\n",
    "\n",
    "3.[Data](Data)\n",
    "\n",
    "4.[Model Trainining ](Model-Trainining)\n",
    "\n",
    "5.[Next Steps](Next-Steps)\n",
    "\n",
    " ## Introduction\n",
    "NVIDIA NeMo is a Conversational AI toolkit. The toolkit is an accelerator, which helps researchers and practitioners to experiments with complex neural network architectures. Speech processing (recognition and synthesis) and Natural Language Processing are the significant capabilities of the platform. As it comes from the NVIDIA, full support to GPU is available. The framework relays on PyTorch as the Deep Learning framework. \n",
    "\n",
    "In this notebook, we will try how to create an Automatic Speech Recognition (ASR). In this tutorial, we will use the LibriSpeech dataset. \n",
    "\n",
    "## Setup\n",
    "\n",
    "For this experiment the following software:\n",
    "Ubuntu 16.04\n",
    "Anaconda 4.7.11\n",
    "NeMo - https://github.com/NVIDIA/NeMo \n",
    "Kaladi - https://github.com/kaldi-asr/kaldi \n",
    "Follow the instructions from the software readme to run the code. \n",
    "Make sure that you have PyTorch installed with GPU support. \n",
    "Hardware Specification\n",
    "Minimum six GiG of GPU RAM is required. \n",
    "\n",
    "## Data\n",
    "The LibriSpeech is an open domain speech recognition dataset.\n",
    "We can download the data from here http://www.openslr.org/12. For this tutorial, we are using the dev-clean dataset - http://www.openslr.org/resources/12/dev-clean.tar.gz .  For making the training easy in a very small GPU footprint, we selected data from the folders 'dev-clean/84/121123/84' and 'dev-clean/84/121550/'. \n",
    "\n",
    "The speech files are store in .flac format, and it should be converted to '.wav' format for NeMo to work. The NeMo training requires a 'manifest' file. The 'manifest' file contains the path to '.wav' (speech recordings), duration of the speech, and transcripts for each recording. \n",
    "\n",
    "To make life easy, we created a utility to convert '.flac' to '.wav' and metadata files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wavconvert import create_nemo_manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Training Manifest File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flac_path = \"/home/jaganadhg/AI_RND/nvidianemo/LibriSpeech/dev-clean/84/121550/\"\n",
    "meta_apth = \"meta_train.json\"\n",
    "\n",
    "create_nemo_manifest(flac_path,\n",
    "    meta_apth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flac_path = \"/home/jaganadhg/AI_RND/nvidianemo/LibriSpeech/dev-clean/84/121123/\"\n",
    "meta_apth = \"meta_val.json\"\n",
    "\n",
    "create_nemo_manifest(flac_path,\n",
    "    meta_apth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainining \n",
    "\n",
    "Let's jump to build a model. We will talk about FFT's, spectrum, and language models later. A utility script is created to abstract the process. The QuartzNet15x5 model is used as the base model. Speech recognition results are evaluated with Word Error Rate (WER). The utility script implements a WER calculator. \n",
    "\n",
    "#### Note- The epoch values to be adjusted accordingly to get a decent model. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asrtrainer import (train_model,\n",
    "        computer_wer)\n",
    "from ruamel.yaml import YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_path = 'quartznet_15x5.yaml'\n",
    "train_manfest = \"metadata.json\"\n",
    "val_manifest = \"metadata_validation.json\"\n",
    "\n",
    "yaml = YAML(typ='safe')\n",
    "with open(config_path) as f:\n",
    "    model_params = yaml.load(f)\n",
    "        \n",
    "my_asr_model = train_model(model_params,\n",
    "                            train_manfest,\n",
    "                            val_manifest,\n",
    "                            5,\n",
    "                            False)\n",
    "    \n",
    "wer = computer_wer(model_params,\n",
    "                    my_asr_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The saved model can be stored to a '.nemo' format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_asr_model.save_to(\"tutorial.nemo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this tutorial, we created a very simple model, which may not be performing well at all. We can try this in building a larger dataset, maybe the entire LibriSpeech dev-clean. An increase in the epochs (I tried with 1000 epochs and transcriptions were looking good!). \n",
    "\n",
    "If you are interested in playing further, the model configurations are available in the 'quartznet_13x5.yaml' file. "
   ]
  }
 ]
}